{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Policy_gradients.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepLearningVision-2019/DLV-Course-Material/blob/master/Notebooks/Reinforcement/Policy_gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-CT_yIGGNs_V",
        "colab_type": "code",
        "outputId": "9c980b47-21b9-4573-ab11-ef1cb218aa0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#remove \" > /dev/null 2>&1\" to see what is going on under the hood\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install gym[box2d] > /dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: setuptools in /usr/local/lib/python3.6/dist-packages (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L4LAiDebN_YE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f16d6525-004f-4636-a597-f064bf8eb97a"
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random, math\n",
        "\n",
        "from keras import models, layers, optimizers\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import glob, io, base64\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "gymlogger.set_level(40) #error only\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "uxuYEwYGOwLT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment and displaying it\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2wgluEHZOxkz",
        "colab_type": "code",
        "outputId": "6a969509-2260-426e-c857-a16c0ef59a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "hg-CkXh5Oz8N",
        "colab_type": "code",
        "outputId": "ec7682c0-3b08-4175-d5ab-28c28e06d32d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# Loads the cartpole environment\n",
        "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "print(state_size, action_size)\n",
        "\n",
        "actions = env.unwrapped.get_action_meanings()\n",
        "\n",
        "# right is up, left is down\n",
        "print(actions)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "n_episodes = 10000\n",
        "\n",
        "print(np.random.choice([2,3]))\n",
        "\n",
        "up_action = 2\n",
        "down_action = 3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "210 6\n",
            "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KcPnb6DwPFxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
        "observation = env.reset()\n",
        "\n",
        "while True:\n",
        "  \n",
        "    env.render()\n",
        "    \n",
        "    #your agent goes here\n",
        "    action = np.random.choice([2,3])\n",
        "    #action = env.action_space.sample() \n",
        "    \n",
        "    observation, reward, done, info = env.step(action) \n",
        "\n",
        "    if done: \n",
        "      break;\n",
        "            \n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZB9VcC_uEpyt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def discount_rewards(reward, gamma):\n",
        "  \n",
        "  r = np.array(reward)\n",
        "  discounted_r = np.zeros_like(r)\n",
        "  running_add = 0\n",
        "    \n",
        "  for t in reversed(range(0, r.size)):\n",
        "    \n",
        "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
        "    running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
        "    discounted_r[t] = running_add\n",
        "  \n",
        "  discounted_r -= np.mean(discounted_r) #normalizing the result\n",
        "  discounted_r /= np.std(discounted_r) #idem\n",
        "  \n",
        "  return discounted_r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6-fAtU4kPJbi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    \n",
        "    def __init__(self, state_size, action_size):\n",
        "      \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "                \n",
        "        self.gamma = 0.99\n",
        "        \n",
        "        self.model = self._build_model()\n",
        "        \n",
        "\n",
        "    def _build_model(self):\n",
        "        \n",
        "        model = models.Sequential()\n",
        "        \n",
        "        model.add(layers.Dense(200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))\n",
        "\n",
        "        \n",
        "        model.add(layers.Dense(1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
        "\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        \n",
        "        return model\n",
        "            \n",
        "    def train(self, states, labels, rewards):\n",
        "        #sample_weight: Optional Numpy array of weights for the training samples, used for weighting the loss function (during training only). \n",
        "        self.model.fit(x=np.vstack(states), y = np.vstack(labels), verbose = 0, sample_weight = discount_rewards(rewards, self.gamma))\n",
        "    \n",
        "           \n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "        \n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "arakY7XfPLSx",
        "colab_type": "code",
        "outputId": "7516f0d8-4485-4a43-b887-55f663b25db8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "agent = DQNAgent(state_size, action_size)\n",
        "agent.model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 200)               1280200   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 1,280,401\n",
            "Trainable params: 1,280,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cqgoYWvLPM4j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocessFrame(image):\n",
        "  \n",
        "  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
        "  image = image[35:195] # crop\n",
        "  image = image[::2,::2,0] # downsample by factor of 2\n",
        "  image[image == 144] = 0 # erase background (background type 1)\n",
        "  image[image == 109] = 0 # erase background (background type 2)\n",
        "  image[image != 0] = 1 # everything else (paddles, ball) just set to 1\n",
        "  \n",
        "  return image.astype(np.float).ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6o17amxPPqD",
        "colab_type": "code",
        "outputId": "0d866432-ce3c-49a3-8d82-b7a22357fb95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
        "running_reward = None\n",
        "\n",
        "try:\n",
        "    for e in range(n_episodes):\n",
        "        \n",
        "        states_train, labels_train, rewards = [], [], []\n",
        "        total_reward = 0\n",
        "        \n",
        "        next_state = env.reset()\n",
        "        prev_state = None\n",
        "        \n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            \n",
        "            #env.render()\n",
        "            current_state = preprocessFrame(next_state)\n",
        "            \n",
        "            delta_state = current_state - prev_state if prev_state is not None else np.zeros(80*80)\n",
        "            \n",
        "            prev_state =  current_state\n",
        "            \n",
        "            prob_up = agent.model.predict(np.expand_dims(delta_state, axis = 1).T)\n",
        "            \n",
        "            action = up_action if np.random.uniform() < prob_up else down_action\n",
        "                                          \n",
        "            label = 1 if action == 2 else 0\n",
        "                                          \n",
        "            states_train.append(delta_state)\n",
        "            labels_train.append(label)\n",
        "            \n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            total_reward += reward\n",
        "                                          \n",
        "        if e % 100 == 0:\n",
        "          running_reward = total_reward if running_reward is None else running_reward * 0.99 + total_reward * 0.01\n",
        "          print('Episode: {} Reward {}'.format(e, total_reward))\n",
        "          agent.save('model_weights_{}_{}_.hdf5'.format(e, running_reward))\n",
        "          \n",
        "        agent.train(states_train, labels_train, rewards)\n",
        "\n",
        "        \n",
        "finally:\n",
        "    env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 0 Reward -21.0\n",
            "Episode: 100 Reward -21.0\n",
            "Episode: 200 Reward -21.0\n",
            "Episode: 300 Reward -21.0\n",
            "Episode: 400 Reward -21.0\n",
            "Episode: 500 Reward -21.0\n",
            "Episode: 600 Reward -21.0\n",
            "Episode: 700 Reward -21.0\n",
            "Episode: 800 Reward -21.0\n",
            "Episode: 900 Reward -21.0\n",
            "Episode: 1000 Reward -21.0\n",
            "Episode: 1100 Reward -21.0\n",
            "Episode: 1200 Reward -21.0\n",
            "Episode: 1300 Reward -21.0\n",
            "Episode: 1400 Reward -21.0\n",
            "Episode: 1500 Reward -21.0\n",
            "Episode: 1600 Reward -21.0\n",
            "Episode: 1700 Reward -21.0\n",
            "Episode: 1800 Reward -21.0\n",
            "Episode: 1900 Reward -21.0\n",
            "Episode: 2000 Reward -21.0\n",
            "Episode: 2100 Reward -21.0\n",
            "Episode: 2200 Reward -21.0\n",
            "Episode: 2300 Reward -21.0\n",
            "Episode: 2400 Reward -21.0\n",
            "Episode: 2500 Reward -21.0\n",
            "Episode: 2600 Reward -21.0\n",
            "Episode: 2700 Reward -21.0\n",
            "Episode: 2800 Reward -21.0\n",
            "Episode: 2900 Reward -21.0\n",
            "Episode: 3000 Reward -21.0\n",
            "Episode: 3100 Reward -21.0\n",
            "Episode: 3200 Reward -21.0\n",
            "Episode: 3300 Reward -21.0\n",
            "Episode: 3400 Reward -21.0\n",
            "Episode: 3500 Reward -21.0\n",
            "Episode: 3600 Reward -21.0\n",
            "Episode: 3700 Reward -21.0\n",
            "Episode: 3800 Reward -21.0\n",
            "Episode: 3900 Reward -21.0\n",
            "Episode: 4000 Reward -21.0\n",
            "Episode: 4100 Reward -21.0\n",
            "Episode: 4200 Reward -21.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fXwY0UIsWC23",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('model_weights.hdf5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4p3CElfPSOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = wrap_env(gym.make('PongDeterministic-v4'))\n",
        "#agent.load('0700hdf5')\n",
        "\n",
        "try:\n",
        "      state = env.reset()\n",
        "      state = np.reshape(state, [1, state_size])\n",
        "\n",
        "      total_reward = 0\n",
        "      done = False\n",
        "\n",
        "      while not done:\n",
        "\n",
        "          env.render()\n",
        "\n",
        "          # Takes a random action from the action space of the environment\n",
        "          action = agent.action(state)\n",
        "\n",
        "          next_state, reward, done, info = env.step(action)\n",
        "\n",
        "          total_reward += reward\n",
        "\n",
        "          next_state = np.reshape(next_state, [1, state_size])\n",
        "          state = next_state\n",
        "        \n",
        "finally:\n",
        "    env.close()       \n",
        "    show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}